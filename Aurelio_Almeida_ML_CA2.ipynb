{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad6e33a",
   "metadata": {},
   "source": [
    "**1. Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfce7c",
   "metadata": {},
   "source": [
    "*Installing the needed libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b0a5e",
   "metadata": {},
   "source": [
    "importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0116f",
   "metadata": {},
   "source": [
    "Step 1: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "data = pd.read_csv('BankRecords.csv')\n",
    "\n",
    "## # Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Display information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Define numeric features for scaling\n",
    "numeric_features = ['Age', 'Experience(Years)', 'Family', 'Credit Score', 'Mortgage(Thousands\\'s)']\n",
    "\n",
    "# Fill missing values in numeric features with the mean value of each column\n",
    "data[numeric_features] = data[numeric_features].fillna(data[numeric_features].mean())\n",
    "\n",
    "# Define categorical features for one-hot encoding\n",
    "categorical_features = ['Education', 'Personal Loan', 'Securities Account', 'CD Account', 'Online Banking', 'CreditCard']\n",
    "\n",
    "# Create a preprocessor with transformers for numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the feature matrix (X) and target vector (y)\n",
    "X = data.drop(['ID', 'Income(Thousands\\'s)'], axis=1)\n",
    "y = data['Income(Thousands\\'s)']\n",
    "\n",
    "# Apply the transformations to the feature matrix\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1be05",
   "metadata": {},
   "source": [
    "Step 2: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46df61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network model\n",
    "nn_model = Sequential([\n",
    "    Input(shape=X_train.shape[1:]), \n",
    "    Dense(64, activation='relu'),   \n",
    "    Dense(64, activation='relu'),    \n",
    "    Dense(1)                         \n",
    "])\n",
    "\n",
    "# Compile the Neural Network model\n",
    "nn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the Neural Network model\n",
    "nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Define and train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457854b",
   "metadata": {},
   "source": [
    "Step 3: Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the Neural Network model\n",
    "y_pred_nn = nn_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error for the Neural Network model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "\n",
    "# Calculate R-squared for the Neural Network model\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "# Predict using the Linear Regression model\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error for the Linear Regression model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "\n",
    "# Calculate R-squared for the Linear Regression model\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print the evaluation metrics for both models\n",
    "print(f'Neural Network MSE: {mse_nn}, R2: {r2_nn}')\n",
    "print(f'Linear Regression MSE: {mse_lr}, R2: {r2_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2511c30",
   "metadata": {},
   "source": [
    "Step 4: Prediction on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new customer data\n",
    "new_customer = {'Age': 30, 'Experience(Years)': 5, 'Family': 3, 'Credit Score': 0.8, 'Mortgage(Thousands\\'s)': 0, \n",
    "                'Education': 'Degree', 'Personal Loan': 'No', 'Securities Account': 'No', 'CD Account': 'No', \n",
    "                'Online Banking': 'Yes', 'CreditCard': 'No'}\n",
    "\n",
    "# Convert the new customer data to a DataFrame\n",
    "new_customer_df = pd.DataFrame([new_customer])\n",
    "\n",
    "# Transform the new customer data using the preprocessor\n",
    "new_customer_transformed = preprocessor.transform(new_customer_df)\n",
    "\n",
    "# Predict the income using the Neural Network model\n",
    "income_prediction_nn = nn_model.predict(new_customer_transformed)\n",
    "\n",
    "# Predict the income using the Linear Regression model\n",
    "income_prediction_lr = lr_model.predict(new_customer_transformed)\n",
    "\n",
    "# Print the predictions for the new customer\n",
    "print(f'Neural Network Prediction for new customer: {income_prediction_nn}')\n",
    "print(f'Linear Regression Prediction for new customer: {income_prediction_lr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7748162",
   "metadata": {},
   "source": [
    "**Data Preparation**\n",
    "\n",
    "Loading and Inspection\n",
    "\n",
    "The dataset 'BankRecords.csv' is loaded using Pandas to examine its structure and contents. This step is crucial for ensuring data integrity and gaining an understanding of the variables present in the dataset.\n",
    "\n",
    "Handling Missing Values\n",
    "\n",
    "Missing values in numeric features such as 'Age', 'Experience(Years)', 'Family', 'Credit Score', and 'Mortgage(Thousands)' are imputed with the mean of their respective columns. This imputation strategy ensures that missing values do not adversely affect the performance of the predictive models.\n",
    "\n",
    "Encoding Categorical Variables\n",
    "\n",
    "Categorical variables including 'Education', 'Personal Loan', 'Securities Account', 'CD Account', 'Online Banking', and 'CreditCard' are encoded using one-hot encoding. This transformation converts categorical variables into a numerical format suitable for model training, ensuring that the models can interpret these variables effectively.\n",
    "\n",
    "Feature Scaling\n",
    "\n",
    "Numeric features are standardized using StandardScaler. Standardization ensures that all features have a mean of 0 and a standard deviation of 1, preventing features with larger scales from dominating the model training process. This step facilitates better convergence during model training.\n",
    "\n",
    "Data Splitting\n",
    "\n",
    "The dataset is split into training and testing sets using the train_test_split function. This division allows for model training on one subset and evaluation on another, enabling an unbiased assessment of model performance and generalization to unseen data.\n",
    "\n",
    "Model Evaluation and Comparison\n",
    "\n",
    "Neural Network Model\n",
    "\n",
    "A neural network model with two hidden layers, each containing 64 neurons, is trained using the Adam optimizer and mean squared error (MSE) loss function. Upon evaluation on the test set, the neural network achieves an MSE of 823.83 and an R2 score of 0.6115.\n",
    "\n",
    "Linear Regression Model\n",
    "\n",
    "A linear regression model is trained and evaluated on the same test set, resulting in an MSE of 926.37 and an R2 score of 0.5631.\n",
    "\n",
    "**Findings and Final Rationale**\n",
    "\n",
    "Both models perform reasonably well in predicting the income of customers. The neural network model slightly outperforms the linear regression model in terms of MSE and R2 score, indicating its ability to capture complex non-linear relationships between features. However, further model tuning and feature engineering may enhance the performance of both models. Techniques such as adjusting neural network architecture, hyperparameter tuning, and feature selection can potentially improve model accuracy and generalization.\n",
    "\n",
    "Prediction for New Customer\n",
    "\n",
    "To predict the income of a new customer not available in the original dataset, their feature values are provided as input to both the trained neural network and linear regression models. The neural network predicts an income of approximately 44,358.21 for the new customer, while the linear regression model predicts an income of approximately 44,774.21. These predictions offer insights into the potential income level of the new customer based on learned patterns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d03457",
   "metadata": {},
   "source": [
    "**2. Semantic Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55031841",
   "metadata": {},
   "source": [
    "*Installing the needed libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas nltk matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f1b02f",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57269e02",
   "metadata": {},
   "source": [
    "Downloading NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385ed2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00923e12",
   "metadata": {},
   "source": [
    "Step 1: Data loading and inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9ddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset #\n",
    "\n",
    "data = pd.read_csv('all_annotated.csv', encoding='latin1')\n",
    "\n",
    "# Display the first few rows of the dataset #\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab79e51",
   "metadata": {},
   "source": [
    "Step 2: Data cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acad57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset to include only English tweets #\n",
    "\n",
    "data_english = data[data['Definitely English'] == 1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b6ec1",
   "metadata": {},
   "source": [
    "Step 3: Defining text pre-processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc94268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words set from NLTK's English stopwords #\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a text preprocessing function #\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<[^>]+>', '', text) \n",
    "    text = re.sub(r'\\W', ' ', text) \n",
    "    text = re.sub(r'\\s+', ' ', text) \n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f69a4e",
   "metadata": {},
   "source": [
    "Step 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff700ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
